<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta name="generator" content="quarto-0.9.272">

  <meta name="author" content="Thanasi Bakis">
  <meta name="dcterms.date" content="2022-06-05">
  <title>Variational Inference</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="presentation_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="presentation_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="presentation_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <script src="presentation_files/libs/clipboard/clipboard.min.js"></script>
  <script src="presentation_files/libs/quarto-html/tabby.min.js"></script>
  <script src="presentation_files/libs/quarto-html/popper.min.js"></script>
  <script src="presentation_files/libs/quarto-html/tippy.umd.min.js"></script>
  <link href="presentation_files/libs/quarto-html/tippy.css" rel="stylesheet">
  <link href="presentation_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet">
  <link id="quarto-text-highlighting-styles" href="presentation_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
  <link href="presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="presentation_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="presentation_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.7em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > :last-child,
  .callout.callout-captioned .callout-body > div > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="center">
  <h1 class="title">Variational Inference</h1>
  <p class="author">Thanasi Bakis</p>
  <p class="date">June 05, 2022</p>
</section>

<section id="the-paper" class="slide level2">
<h2>The paper</h2>
<p>Blei, D. M., Kucukelbir, A., &amp; McAuliffe, J. D. (2017). Variational inference: A review for statisticians. <em>Journal of the American statistical Association</em>, 112(518), 859-877.</p>
</section>
<section id="big-idea" class="slide level2">
<h2>Big idea</h2>
<p>Variational inference (VI) is a technique for approximating intractable distributions.</p>
<p><br>
</p>
<p>This arises frequently in Bayesian statistics, when a posterior density cannot be analytically evaluated due to an intractable marginal likelihood:</p>
<p><span class="math display">\[
p(\mathbf{z} | \mathbf{x}) = \frac{p(\mathbf{z}) p(\mathbf{x} | \mathbf{z})}{\int p(\mathbf{z}) p(\mathbf{x} | \mathbf{z}) \, d\mathbf{z}}
\]</span></p>
<p><br>
</p>
<p>Sound familiar?</p>
</section>
<section id="sounds-like-mcmc" class="slide level2">
<h2>Sounds like MCMC…</h2>
<p>MCMC and VI are two tools for the same job.</p>
<p><br>
</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>MCMC</strong></p>
<div class="fragment" data-fragment-index="1">
<ul>
<li>Constructs a Markov chain of samples</li>
</ul>
</div>
<div class="fragment" data-fragment-index="3">
<ul>
<li>Heavy computational cost</li>
</ul>
</div>
<div class="fragment" data-fragment-index="5">
<ul>
<li>Convergence guarantees to the target (Ergodic theorem)</li>
</ul>
</div>
</div><div class="column" style="width:50%;">
<p><strong>VI</strong></p>
<div class="fragment" data-fragment-index="2">
<ul>
<li>Produces a density function</li>
</ul>
</div>
<div class="fragment" data-fragment-index="4">
<ul>
<li>Tends to be faster than MCMC</li>
</ul>
</div>
<div class="fragment" data-fragment-index="6">
<ul>
<li>:-(</li>
</ul>
</div>
</div>
</div>
<p><br>
</p>
<div class="fragment">
<p>While VI lacks in exactness, it can make Bayesian inference possible in scenarios where MCMC is impractical <em>(eg. very large data sets, deep learning)</em>.</p>
</div>
</section>
<section id="overview-of-vi" class="slide level2">
<h2>Overview of VI</h2>
<p>Goal: approximate <span class="math inline">\(p(\mathbf{z} | \mathbf{x}) \propto p(\mathbf{z}) p(\mathbf{x} | \mathbf{z})\)</span></p>
<p><br>
</p>
<p>Key steps:</p>
<ul>
<li>Propose a family of distributions <span class="math inline">\(\mathcal{Q}\)</span> to approximate the target <span class="math inline">\(p(\mathbf{z} | \mathbf{x})\)</span></li>
<li>Choose the member <span class="math inline">\(q(\mathbf{z})\)</span> that “best approximates” the target
<ul>
<li>Called the <em>variational distribution</em></li>
</ul></li>
</ul>
<p><br>
</p>
<p>The family can be some parametric form, eg. <span class="math inline">\(\mathcal{Q} = \{ N(\mu, \sigma^2) : \mu \in \mathbb{R}, \sigma^2 &gt; 0 \}\)</span>.</p>
<p>In this case, choosing the “best” <span class="math inline">\(q\)</span> distribution amounts to choosing the “best” parameter values.</p>
<p>These parameters are called the <em>variational parameters</em>, <span class="math inline">\(\mathbf{\phi}\)</span>.</p>
</section>
<section id="what-is-best" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">What is “best”?</h2>
<p>Specifically, we choose the <span class="math inline">\(q\)</span> that is the closest to the target, in terms of KL divergence:</p>
<p><span class="math display">\[
q^*(\mathbf{z}) = \underset{q \in \mathcal{Q}}{\text{arg min}} \, \text{KL} \big( q(\mathbf{z}) \, || \, p(\mathbf{z} | \mathbf{x}) \big)
\]</span></p>
<p><br>
</p>
<p>This might not look easy, since it isn’t obvious how to measure divergence involving the posterior if we don’t know the posterior…</p>
<p><br>
</p>
<p>Similar to MCMC (Metropolis-Hastings ratio), knowing the posterior <em>up to a constant</em> is good enough!</p>
</section>
<section id="what-is-best-1" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">What is “best”?</h2>
<p>Specifically, we choose the <span class="math inline">\(q\)</span> that is the closest to the target, in terms of KL divergence:</p>
<p><span class="math display">\[
q^*(\mathbf{z}) = \underset{q \in \mathcal{Q}}{\text{arg min}} \, \text{KL} \big( q(\mathbf{z}) \, || \, p(\mathbf{z} | \mathbf{x}) \big)
\]</span></p>
<p><br>
</p>
<p>Similar to MCMC (Metropolis-Hastings ratio), knowing the posterior <em>up to a constant</em> is good enough!</p>
<p><br>
</p>
<p><span class="math display">\[
\begin{aligned}
\text{KL} \big( q(\mathbf{z}) \, || \, p(\mathbf{z} | \mathbf{x}) \big)
&amp;= \mathbb{E}_{\mathbf{z} \sim q} \big[ \log q(\mathbf{z}) - \log p(\mathbf{z} | \mathbf{x}) \big] \\
&amp;= \mathbb{E}_{\mathbf{z} \sim q} \big[ \log q(\mathbf{z}) - \log p(\mathbf{z}, \mathbf{x}) \big] + \log p(\mathbf{x})
\end{aligned}
\]</span></p>
</section>
<section id="what-is-best-2" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">What is “best”?</h2>
<p>Specifically, we choose the <span class="math inline">\(q\)</span> that is the closest to the target, in terms of KL divergence:</p>
<p><span class="math display">\[
q^*(\mathbf{z}) = \underset{q \in \mathcal{Q}}{\text{arg min}} \, \text{KL} \big( q(\mathbf{z}) \, || \, p(\mathbf{z} | \mathbf{x}) \big)
\]</span></p>
<p><br>
</p>
<p><span class="math display">\[
\begin{aligned}
\text{KL} \big( q(\mathbf{z}) \, || \, p(\mathbf{z} | \mathbf{x}) \big)
&amp;= \mathbb{E}_{\mathbf{z} \sim q} \big[ \log q(\mathbf{z}) - \log p(\mathbf{z} | \mathbf{x}) \big] \\
&amp;= \mathbb{E}_{\mathbf{z} \sim q} \big[ \log q(\mathbf{z}) - \log p(\mathbf{z}, \mathbf{x}) \big] + \log p(\mathbf{x})
\end{aligned}
\]</span></p>
<p><br>
</p>
<p>Minimizing the KL is then equivalent to minimizing this expectation.</p>
<ul>
<li>We know the joint distribution, and choose the variational distribution</li>
<li><span class="math inline">\(\log p(\mathbf{x})\)</span> is constant w.r.t. <span class="math inline">\(q\)</span></li>
</ul>
</section>
<section id="introducing-the-elbo" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Introducing the ELBO</h2>
<p><span class="math display">\[
\begin{aligned}
\text{KL} \big( q(\mathbf{z}) \, || \, p(\mathbf{z} | \mathbf{x}) \big)
&amp;= \mathbb{E}_{\mathbf{z} \sim q} \big[ \log q(\mathbf{z}) - \log p(\mathbf{z} | \mathbf{x}) \big] \\
&amp;= \mathbb{E}_{\mathbf{z} \sim q} \big[ \log q(\mathbf{z}) - \log p(\mathbf{z}, \mathbf{x}) \big] + \log p(\mathbf{x})
\end{aligned}
\]</span></p>
<p><br>
</p>
<p>Instead of minimizing the expectation, the VI literature maximizes its negative, and calls this quantity the <em>evidence lower bound</em> (ELBO):</p>
<p><span class="math display">\[
\text{ELBO}(q) := \mathbb{E}_{\mathbf{z} \sim q} \big[ \log p(\mathbf{z}, \mathbf{x}) - \log q(\mathbf{z}) \big] \\
\]</span></p>
<p>Then:</p>
<p><span class="math display">\[
\text{KL} \big( q(\mathbf{z}) \, || \, p(\mathbf{z} | \mathbf{x}) \big) = -\text{ELBO(q)} + \log p(\mathbf{x})
\]</span></p>
</section>
<section id="introducing-the-elbo-1" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Introducing the ELBO</h2>
<p>Instead of minimizing the expectation, the VI literature maximizes its negative, and calls this quantity the <em>evidence lower bound</em> (ELBO):</p>
<p><span class="math display">\[
\text{ELBO}(q) := \mathbb{E}_{\mathbf{z} \sim q} \big[ \log p(\mathbf{z}, \mathbf{x}) - \log q(\mathbf{z}) \big] \\
\]</span></p>
<p>Then:</p>
<p><span class="math display">\[
\text{KL} \big( q(\mathbf{z}) \, || \, p(\mathbf{z} | \mathbf{x}) \big) = -\text{ELBO(q)} + \log p(\mathbf{x})
\]</span></p>
<p><br>
</p>
<p>Why the name?</p>
<p><span class="math display">\[
\begin{aligned}
\text{ELBO(q)}
&amp;= \log p(\mathbf{x}) - \text{KL} \big( q(\mathbf{z}) \, || \, p(\mathbf{z} | \mathbf{x}) \big) \\
&amp;\leq \log p(\mathbf{x})
\end{aligned}
\]</span></p>
</section>
<section id="introducing-the-elbo-2" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Introducing the ELBO</h2>
<p>Instead of minimizing the expectation, the VI literature maximizes its negative, and calls this quantity the <em>evidence lower bound</em> (ELBO):</p>
<p><span class="math display">\[
\text{ELBO}(q) := \mathbb{E}_{\mathbf{z} \sim q} \big[ \log p(\mathbf{z}, \mathbf{x}) - \log q(\mathbf{z}) \big]
\]</span></p>
<p><br>
</p>
<p>Fun fact about the ELBO: it can be rewritten and interpreted as a balance of two objectives…</p>
<ul>
<li>Maximize the expected likelihood (explain the data)</li>
<li>Minimize prior divergence (regularize)</li>
</ul>
<p><span class="math display">\[
\text{ELBO}(q) = \mathbb{E}_{\mathbf{z} \sim q} \big[ \log p(\mathbf{x} | \mathbf{z}) \big] - \text{KL} \big( q(\mathbf{z}) \, || \, p(\mathbf{z}) \big)
\]</span></p>
</section>
<section id="the-mean-field-approximation" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">The mean-field approximation</h2>
<p>As a review, the objective is:</p>
<p><span class="math display">\[
\underset{q \in \mathcal{Q}}{\text{arg max}} \, \text{ELBO}(q) = \underset{q \in \mathcal{Q}}{\text{arg max}} \, \mathbb{E}_{\mathbf{z} \sim q} \big[ \log p(\mathbf{z}, \mathbf{x}) - \log q(\mathbf{z}) \big]
\]</span></p>
<p><br>
</p>
<p>One very common type of <span class="math inline">\(\mathcal{Q}\)</span> we propose is called the <em>mean-field family</em>. Essentially, it only contains distributions that enforce independence between latent variables:</p>
<p><span class="math display">\[
q(\mathbf{z}) = q_1(z_1) \cdot q_2(z_2) \cdot q_3(z_3) \cdot \ldots
\]</span></p>
<p><br>
</p>
<p><em>This is a trade-off: we lose flexibility in our approximation, but we gain a simpler optimization landscape.</em></p>
</section>
<section id="the-mean-field-approximation-1" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">The mean-field approximation</h2>
<p><em>This is a trade-off: we lose flexibility in our approximation, but we gain a simpler optimization landscape.</em></p>

<img data-src="images/tradeoff.png" class="r-stretch"><p>eg. If we target a bivariate normal <em>with correlation</em>, the best we can do is a diagonal covariance matrix, since the mean-field approximation forces posterior independence.</p>
<aside class="notes">
<p>The green circle could be stretched along either axis to adjust the marginal variance of that latent variable, but we won’t see any stretching in the diagonal direction, since that is modeled by covariance.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="an-example" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p>Let’s explore a Gaussian mixture model (known variance).</p>
<ul>
<li><span class="math inline">\(\mu_k \in \mathbb{R}\)</span> is the mean of the <span class="math inline">\(k^{th}\)</span> class
<ul>
<li><span class="math inline">\(\mathbf{\mu} = (\mu_1, \ldots, \mu_K)'\)</span></li>
</ul></li>
<li><span class="math inline">\(\mathbf{c_i}\)</span> is a <span class="math inline">\(Kx1\)</span> vector of 0s, except for value 1 at some index <span class="math inline">\(k\)</span>
<ul>
<li>Indicates assignment of <span class="math inline">\(x_i\)</span> to the <span class="math inline">\(k^{th}\)</span> class, for <span class="math inline">\(i = 1...n\)</span></li>
</ul></li>
</ul>
<p><br>
</p>
<p>Then,</p>
<p><span class="math display">\[
x_i | \mathbf{c_i}, \mu \sim N(\mathbf{c_i}' \mu, 1)
\]</span></p>
<p>for <span class="math inline">\(i = 1...n\)</span>.</p>
</section>
<section id="an-example-1" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p>Gaussian mixture model:</p>
<p><span class="math display">\[
x_i | \mathbf{c_i}, \mu \sim N(\mathbf{c_i}' \mu, 1)
\]</span></p>
<p><br>
</p>
<p>In the Bayesian setting, we place priors on the latent variables:</p>
<p><span class="math display">\[
\begin{aligned}
\mu_k &amp;\sim N(0, \sigma^2) \qquad &amp;k &amp;= 1...K \\
\mathbf{c_i} &amp;\sim \text{Categorical}(\frac{1}{K}, ..., \frac{1}{K}) &amp;i &amp;= 1...n
\end{aligned}
\]</span></p>
<p>The goal is to do inference on <span class="math inline">\(\mathbf{z} := \{\mu_1, \ldots, \mu_K, \mathbf{c_1}, \ldots, \mathbf{c_n}\}\)</span>.</p>
<aside class="notes">
<p><span class="math inline">\(\sigma^2\)</span> is a hyperparameter</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="an-example-2" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p>The goal is to do inference on <span class="math inline">\(\mathbf{z} := \{\mu_1, \ldots, \mu_K, \mathbf{c_1}, \ldots, \mathbf{c_n}\}\)</span>.</p>
<p><br>
</p>
<p>We propose the following mean-field variational family:</p>
<p><span class="math display">\[
q(\mathbf{z}; \mathbf{\phi}) = \prod_{i=1}^K q(\mu_k; m_k, s_k^2) \prod_{i=1}^n q(\mathbf{c_i}; \mathbf{\pi_i})
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(q(\mu_k; m_k, s_k^2) = N(m_k, s_k^2)\)</span></li>
<li><span class="math inline">\(q(\mathbf{c_i}; \mathbf{\pi_i}) = Categorical(\mathbf{\pi_i})\)</span>
<ul>
<li><span class="math inline">\(\mathbf{\pi_i}\)</span> is a Kx1 vector of class assignment probabilities</li>
<li>ie. <span class="math inline">\(\pi_{ik}\)</span> is the probability that <span class="math inline">\(c_{ik} = 1\)</span> (and all other entries in <span class="math inline">\(\mathbf{c_i}\)</span> are <span class="math inline">\(0\)</span>)</li>
</ul></li>
</ul>
<p>Variational parameters <span class="math inline">\(\mathbf{\phi} = \{ m_1, \ldots, m_K, s_1^2, \ldots, s_K^2, \mathbf{\pi_1}, \ldots, \mathbf{\pi_K} \}\)</span></p>
<aside class="notes">
<p>Blei (2018): <em>In fact, these are the optimal forms of the mean-field variational density for the mixture of Gaussians.</em></p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="an-example-3" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p>The goal is to do inference on <span class="math inline">\(\mathbf{z} := \{\mu_1, \ldots, \mu_K, \mathbf{c_1}, \ldots, \mathbf{c_n}\}\)</span>.</p>
<p><br>
</p>
<p>We propose the following mean-field variational family:</p>
<p><span class="math display">\[
q(\mathbf{z}; \mathbf{\phi}) = \prod_{i=1}^K q(\mu_k; m_k, s_k^2) \prod_{i=1}^n q(\mathbf{c_i}; \mathbf{\pi_i})
\]</span></p>
<p>Variational parameters <span class="math inline">\(\mathbf{\phi} = \{ m_1, \ldots, m_K, s_1^2, \ldots, s_K^2, \mathbf{\pi_1}, \ldots, \mathbf{\pi_K} \}\)</span></p>
<p><br>
</p>
<p>Objective: <span class="math inline">\(\underset{q}{\text{arg max}} \, \text{ELBO}(q) = \underset{\mathbf{\phi}}{\text{arg max}} \, \text{ELBO}(q)\)</span></p>
<p>ie. Finding the optimal <span class="math inline">\(q\)</span> in this family amounts to finding the optimal variational parameters.</p>
</section>
<section id="an-example-4" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p>Objective: <span class="math inline">\(\underset{q}{\text{arg max}} \, \text{ELBO}(q) = \underset{\mathbf{\phi}}{\text{arg max}} \, \text{ELBO}(q)\)</span></p>
<p><br>
</p>
<p>To perform this optimization, we are going to leverage the mean-field assumption and play a trick…</p>
<p><span class="math display">\[
\begin{aligned}
\text{ELBO}(q)
&amp;= \mathbb{E}_{\mathbf{z} \sim q} \big[ \log p(\mathbf{z}, \mathbf{x}) - \log q(\mathbf{z}) \big] \\
&amp;= \mathbb{E}_{z_j \sim q_j} \big[ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} \big[ \log p(\mathbf{z}, \mathbf{x}) \big] \big] - \mathbb{E}_{\mathbf{z} \sim q} \big[ \log q(z_j) \big] + \text{const. w.r.t. } z_j  \\
&amp;= \mathbb{E}_{z_j \sim q_j} \big[ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} \big[ \log p(\mathbf{z}, \mathbf{x}) \big] \big] - \mathbb{E}_{z_j \sim q_j} \big[ \log q(z_j) \big] + C  \\
&amp;= \mathbb{E}_{z_j \sim q_j} \big[ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} \big[ \log p(\mathbf{z}, \mathbf{x}) \big] - \log q(z_j) \big] + C  \\
&amp;= -\text{KL}(q(z_j) \, || \, e^{ \{ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} [ \log p(\mathbf{z}, \mathbf{x}) ] \} }) + C
\end{aligned}
\]</span></p>
<aside class="notes">
<p><span class="math inline">\(q_j\)</span> is the factor for <span class="math inline">\(z_j\)</span> in the mean-field posterior, and <span class="math inline">\(q_{-j}\)</span> is the product of all the other factors leftover</p>
<p>In step two term 1, the independence assumption lets <span class="math inline">\(q\)</span> be factorized into separate integrals/expectations. In step two term 2, it lets us split up <span class="math inline">\(q(z_j)\)</span> from the other terms</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="an-example-5" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p><span class="math display">\[
\begin{aligned}
\text{ELBO}(q)
&amp;= \mathbb{E}_{\mathbf{z} \sim q} \big[ \log p(\mathbf{z}, \mathbf{x}) - \log q(\mathbf{z}) \big] \\
&amp;= \mathbb{E}_{z_j \sim q_j} \big[ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} \big[ \log p(\mathbf{z}, \mathbf{x}) \big] \big] - \mathbb{E}_{\mathbf{z} \sim q} \big[ \log q(z_j) \big] + \text{const. w.r.t. } z_j  \\
&amp;= \mathbb{E}_{z_j \sim q_j} \big[ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} \big[ \log p(\mathbf{z}, \mathbf{x}) \big] \big] - \mathbb{E}_{z_j \sim q_j} \big[ \log q(z_j) \big] + C  \\
&amp;= \mathbb{E}_{z_j \sim q_j} \big[ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} \big[ \log p(\mathbf{z}, \mathbf{x}) \big] - \log q(z_j) \big] + C  \\
&amp;= -\text{KL}(q(z_j) \, || \, e^{ \{ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} [ \log p(\mathbf{z}, \mathbf{x}) ] \} }) + C
\end{aligned}
\]</span></p>
<p><br>
</p>
<p>Let’s freeze all variational parameters except the ones involved in <span class="math inline">\(q_j(z_j)\)</span>.</p>
<p><br>
</p>
<p>Then maximizing the ELBO boils down to minimzing that KL divergence… which is easy, since KL divergence is minimized when the two distributions are equal/proportional.</p>
</section>
<section id="an-example-6" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p><span class="math display">\[
\begin{aligned}
\text{ELBO}(q)
&amp;= \mathbb{E}_{\mathbf{z} \sim q} \big[ \log p(\mathbf{z}, \mathbf{x}) - \log q(\mathbf{z}) \big] \\
&amp;= \mathbb{E}_{z_j \sim q_j} \big[ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} \big[ \log p(\mathbf{z}, \mathbf{x}) \big] \big] - \mathbb{E}_{\mathbf{z} \sim q} \big[ \log q(z_j) \big] + \text{const. w.r.t. } z_j  \\
&amp;= \mathbb{E}_{z_j \sim q_j} \big[ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} \big[ \log p(\mathbf{z}, \mathbf{x}) \big] \big] - \mathbb{E}_{z_j \sim q_j} \big[ \log q(z_j) \big] + C  \\
&amp;= \mathbb{E}_{z_j \sim q_j} \big[ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} \big[ \log p(\mathbf{z}, \mathbf{x}) \big] - \log q(z_j) \big] + C  \\
&amp;= -\text{KL}(q(z_j) \, || \, e^{ \{ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} [ \log p(\mathbf{z}, \mathbf{x}) ] \} }) + C
\end{aligned}
\]</span></p>
<p><br>
</p>
<p>The optimal <span class="math inline">\(q\)</span> has all of the frozen <span class="math inline">\(q_{-j}\)</span> terms, and updates <span class="math inline">\(q_j\)</span>’s parameters according to:</p>
<p><span class="math display">\[
q_j^*(z_j) \propto e^{ \{ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} [ \log p(\mathbf{z}, \mathbf{x}) ] \} }
\]</span></p>
<aside class="notes">
<p>This will make more sense in the context of our example…</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="an-example-7" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p>The optimal <span class="math inline">\(q\)</span> has all of the frozen <span class="math inline">\(q_{-j}\)</span> terms, and updates <span class="math inline">\(q_j\)</span>’s parameters according to:</p>
<p><span class="math display">\[
q_j^*(z_j) \propto e^{ \{ \mathbb{E}_{\mathbf{z_{-j}} \sim q_{-j}} [ \log p(\mathbf{z}, \mathbf{x}) ] \} }
\]</span></p>
<p><br>
</p>
<p>Thus, to maximize the ELBO for all variational parameters, we have an iterative algorithm.</p>
<p><br>
</p>
<p>For each element <span class="math inline">\(z_j\)</span> of <span class="math inline">\(\mathbf{z}\)</span>, we perform the above update, freezing the parameters belonging to the other elements.</p>
</section>
<section id="an-example-8" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p>Let’s see this in the context of our example.</p>
<p><br>
</p>
<p>Start with the term for <span class="math inline">\(\mathbf{c_i}\)</span>, fixing <span class="math inline">\(\mathbf{z}_{-\mathbf{c_i}} := \{ \mathbf{c_1}, \ldots, \mathbf{c_{i-1}}, \mathbf{c_{i+1}}, \ldots, \mathbf{c_n}, \mu_1, \ldots, \mu_K \}\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
q_{\mathbf{c_i}}^*(\mathbf{c_i}; \mathbf{\pi_i})
&amp;\propto \exp \big\{ \mathbb{E}_{\mathbf{z}_{-\mathbf{c_i}} \sim q_{-\mathbf{c_i}}} \big[ \log p(\mathbf{z}, \mathbf{x}) \big] \big\} \\
&amp;\propto \exp \big\{ \log p(\mathbf{c_i}) + \mathbb{E}_{\mathbf{z}_{-\mathbf{c_i}} \sim q_{-\mathbf{c_i}}} \big[ \log p(x_i | \mathbf{c_i}, \mathbf{\mu}) \big] \big\} \\
&amp;= \exp \big\{ \log p(\mathbf{c_i}) + \mathbb{E}_{\mathbf{z}_{-\mathbf{c_i}} \sim q_{-\mathbf{c_i}}} \big[ \sum_{k=1}^K c_{ik} \log p(x_i | \mathbf{c_i}, \mu_k) \big] \big\} \\
&amp;\ldots \\
&amp;\propto \exp \big\{ \log \frac{1}{K} + \sum_{k=1}^K c_{ik} \left( \mathbb{E}_{q_{\mu_k}} [\mu_k] x_i - \frac{1}{2} \mathbb{E}_{q_{\mu_k}} [\mu_k^2] \right) \big\}
\end{aligned}
\]</span></p>
<aside class="notes">
<p>Even though we start with <span class="math inline">\(p(\mathbf{z}, \mathbf{x})\)</span>, we drop anything that doesn’t have to do with data point <span class="math inline">\(i\)</span>, since this variational factor is the posterior for class assignment <span class="math inline">\(i\)</span> marginally</p>
<p>Note how we expand <span class="math inline">\(\log p(x_i | \mathbf{c_i}, \mathbf{\mu})\)</span> to have sums of terms with each <span class="math inline">\(\mu_k\)</span>, using the indicator elements of <span class="math inline">\(\mathbf{c_i}\)</span> to zero out the means that don’t belong to class <span class="math inline">\(i\)</span></p>
<p>The <span class="math inline">\(\ldots\)</span> is just subbing in the log normal density and simplifying constants. Also plugging in discrete uniform prior on <span class="math inline">\(\mathbf{c_i}\)</span>, which is constant with respect to <span class="math inline">\(i\)</span> so it drops out in the end</p>
<p>Also at the end we replace the expectations with the mean and second moment from the variational normal distribution for <span class="math inline">\(q_{\mu_k}(\mu_k)\)</span></p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="an-example-9" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p>Start with the term for <span class="math inline">\(\mathbf{c_i}\)</span>, fixing <span class="math inline">\(\mathbf{z}_{-\mathbf{c_i}} := \{ \mathbf{c_1}, \ldots, \mathbf{c_{i-1}}, \mathbf{c_{i+1}}, \ldots, \mathbf{c_n}, \mu_1, \ldots, \mu_K \}\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
q_{\mathbf{c_i}}^*(\mathbf{c_i}; \mathbf{\pi_i})
&amp;\propto \exp \big\{ \mathbb{E}_{\mathbf{z}_{-\mathbf{c_i}} \sim q_{-\mathbf{c_i}}} \big[ \log p(\mathbf{z}, \mathbf{x}) \big] \big\} \\
&amp;\propto \exp \big\{ \log p(\mathbf{c_i}) + \mathbb{E}_{\mathbf{z}_{-\mathbf{c_i}} \sim q_{-\mathbf{c_i}}} \big[ \log p(x_i | \mathbf{c_i}, \mathbf{\mu}) \big] \big\} \\
&amp;= \exp \big\{ \log p(\mathbf{c_i}) + \mathbb{E}_{\mathbf{z}_{-\mathbf{c_i}} \sim q_{-\mathbf{c_i}}} \big[ \sum_{k=1}^K c_{ik} \log p(x_i | \mathbf{c_i}, \mu_k) \big] \big\} \\
&amp;\ldots \\
&amp;\propto \exp \big\{ \log \frac{1}{K} + \sum_{k=1}^K c_{ik} \left( \mathbb{E}_{q_{\mu_k}} [\mu_k] x_i - \frac{1}{2} \mathbb{E}_{q_{\mu_k}} [\mu_k^2] \right) \big\}
\end{aligned}
\]</span></p>
<p><br>
</p>
<p>The kernel of the optimal posterior term is <span class="math inline">\(q_{\mathbf{c_i}}^*(\mathbf{c_i}; \mathbf{\pi_i}) \propto \prod_{k=1}^K e^{ c_{ik} \left( m_k x_i - \frac{1}{2} (m_k^2 + s_k^2) \right) }\)</span></p>
</section>
<section id="an-example-10" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p>The kernel of the optimal posterior term is <span class="math inline">\(q_{\mathbf{c_i}}^*(\mathbf{c_i}; \mathbf{\pi_i}) \propto \prod_{k=1}^K e^{ c_{ik} \left( m_k x_i - \frac{1}{2} (m_k^2 + s_k^2) \right) }\)</span></p>
<p><br>
</p>
<p>Remember that we specified the family of <span class="math inline">\(q_{\mathbf{c_i}}^*(\mathbf{c_i}; \mathbf{\pi_i})\)</span> to be a categorical distribution over the <span class="math inline">\(K\)</span> possible vectors <span class="math inline">\(\mathbf{c_i}\)</span> (one for each element that could be <span class="math inline">\(1\)</span>).</p>
<p><br>
</p>
<p>By definition, the categorical distribution’s PMF is:</p>
<p><span class="math display">\[
q_{\mathbf{c_i}}^*(\mathbf{c_i}; \mathbf{\pi_i}) = \prod_{\text{possible values } \mathbf{c'} \text{ of } \mathbf{c_i}} \{ \text{element of } \mathbf{\pi_i} \text{ corresponding to } \mathbf{c'} \}^{\mathbb{I}(\mathbf{c_i} = \mathbf{c'})}
\]</span></p>
</section>
<section id="an-example-11" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p>The kernel of the optimal posterior term is <span class="math inline">\(q_{\mathbf{c_i}}^*(\mathbf{c_i}; \mathbf{\pi_i}) \propto \prod_{k=1}^K e^{ c_{ik} \left( m_k x_i - \frac{1}{2} (m_k^2 + s_k^2) \right) }\)</span></p>
<p><br>
</p>
<p>By definition, the categorical distribution’s PMF is:</p>
<p><span class="math display">\[
q_{\mathbf{c_i}}^*(\mathbf{c_i}; \mathbf{\pi_i}) = \prod_{\text{possible values } \mathbf{c'} \text{ of } \mathbf{c_i}} \{ \text{element of } \mathbf{\pi_i} \text{ corresponding to } \mathbf{c'} \}^{\mathbb{I}(\mathbf{c_i} = \mathbf{c'})}
\]</span></p>
<p><br>
</p>
<p>Each possible value of <span class="math inline">\(\mathbf{c_i}\)</span> corresponds to a unique index <span class="math inline">\(k = 1...K\)</span> containing the value 1 in <span class="math inline">\(c_i\)</span>, so we can combine the two:</p>
<p><span class="math display">\[
\begin{aligned}
q_{\mathbf{c_i}}^*(\mathbf{c_i}; \mathbf{\pi_i})\
&amp;\propto \prod_{k=1}^K e^{ c_{ik} \left( m_k x_i - \frac{1}{2} (m_k^2 + s_k^2) \right) } \\
&amp;= \prod_{k=1}^K \left( e^{ m_k x_i - \frac{1}{2} (m_k^2 + s_k^2) } \right)^{c_{ik}} \\
\end{aligned}
\]</span></p>
</section>
<section id="an-example-12" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p>By definition, the categorical distribution’s PMF is:</p>
<p><span class="math display">\[
q_{\mathbf{c_i}}^*(\mathbf{c_i}; \mathbf{\pi_i}) = \prod_{\text{possible values } \mathbf{c'} \text{ of } \mathbf{c_i}} \{ \text{element of } \mathbf{\pi_i} \text{ corresponding to } \mathbf{c'} \}^{\mathbb{I}(\mathbf{c_i} = \mathbf{c'})}
\]</span></p>
<p><br>
</p>
<p>Each possible value of <span class="math inline">\(\mathbf{c_i}\)</span> corresponds to a unique index <span class="math inline">\(k = 1...K\)</span> containing the value 1 in <span class="math inline">\(c_i\)</span>, so we can combine the two:</p>
<p><span class="math display">\[
\begin{aligned}
q_{\mathbf{c_i}}^*(\mathbf{c_i}; \mathbf{\pi_i})\
&amp;\propto \prod_{k=1}^K e^{ c_{ik} \left( m_k x_i - \frac{1}{2} (m_k^2 + s_k^2) \right) } \\
&amp;= \prod_{k=1}^K \left( e^{ m_k x_i - \frac{1}{2} (m_k^2 + s_k^2) } \right)^{c_{ik}} \\
\end{aligned}
\]</span></p>
<p>Therefore, the optimal updated <span class="math inline">\(\pi_{ik} \propto e^{ m_k x_i - \frac{1}{2} (m_k^2 + s_k^2) }\)</span>.</p>
</section>
<section id="an-example-13" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p>Therefore, the optimal updated <span class="math inline">\(\pi_{ik} \propto e^{ m_k x_i - \frac{1}{2} (m_k^2 + s_k^2) }\)</span>.</p>
<p><br>
</p>
<p>Thus, in one iteration of the optimization loop, for every data point <span class="math inline">\(i\)</span>, we will compute <span class="math inline">\(\tilde{\pi}_{ik} = e^{ m_k x_i - \frac{1}{2} (m_k^2 + s_k^2) }\)</span> and then normalize the <span class="math inline">\(\tilde{\pi}_{ik}\)</span> across <span class="math inline">\(k\)</span> (fixing <span class="math inline">\(i\)</span>):</p>
<p><span class="math display">\[
\pi_{ik}
= \frac{\tilde{\pi}_{ik}}{\sum_{k'=1}^K \tilde{\pi}_{ik'}}
= \frac{e^{ m_k x_i - \frac{1}{2} (m_k^2 + s_k^2) }}{\sum_{k'=1}^K e^{ m_{k'} x_i - \frac{1}{2} (m_{k'}^2 + s_{k'}^2) }}
\]</span></p>
<p><br>
</p>
<p><em>(Normalization is required because we only have values proportional to <span class="math inline">\(\pi_{ik}\)</span>, and we need them to be valid probabilities [0, 1].)</em></p>
</section>
<section id="an-example-14" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">An example</h2>
<p><span class="math display">\[
\pi_{ik}
= \frac{\tilde{\pi}_{ik}}{\sum_{k'=1}^K \tilde{\pi}_{ik'}}
= \frac{e^{ m_k x_i - \frac{1}{2} (m_k^2 + s_k^2) }}{\sum_{k'=1}^K e^{ m_{k'} x_i - \frac{1}{2} (m_{k'}^2 + s_{k'}^2) }}
\]</span></p>
<p><br>
</p>
<p>To spare you some math, we can show similarly for other variational parameters that each iteration of the optimization loop will update:</p>
<p><span class="math display">\[
\begin{aligned}
m_k &amp;= \frac{\sum_i \pi_{ik} x_i}{1/\sigma^2 + \sum_i \pi_{ik}} \\
s_k^2 &amp;= \frac{1}{1/\sigma^2 + \sum_i \pi_{ik}}
\end{aligned}
\]</span></p>
</section>
<section id="an-example-15" class="slide level2">
<h2>An example</h2>

<img data-src="images/algorithm2.png" class="r-stretch"><aside class="notes">
<p>Note they use <span class="math inline">\(\phi\)</span> here where I used <span class="math inline">\(\pi\)</span></p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="an-example-16" class="slide level2">
<h2>An example</h2>
<p>Simulation study with two-dimensional Gaussian mixture model, <span class="math inline">\(K = 5\)</span>.</p>

<img data-src="images/gmm_result.png" class="r-stretch"><p>Notice how, even as the approximation converges, each bivariate normal posterior cannot be “diagonal” due to the mean-field independence assumption.</p>
<aside class="notes">
<p>“The elipses are <span class="math inline">\(2 \sigma\)</span> contours of the variational approximating factors”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="whats-next-for-vi" class="slide level2">
<h2>What’s next for VI?</h2>
<ul>
<li>Different divergences beyond KL?
<ul>
<li>The ELBO is specific to KL, but perhaps a different divergence would be a tighter lower-bound to the evidence, yielding a better approximation</li>
</ul></li>
<li>Relaxing mean-field assumption without sacrificing too much efficiency?</li>
<li>Learning/optimizing the form of the variational distribution along with the parameters?</li>
<li>Studying statistical properties and guarantees?
<ul>
<li>“Understanding VI as an estimator”</li>
</ul></li>
<li>Combining MCMC and VI for an ideal tradeoff between accuracy and speed?</li>
</ul>
</section>
<section id="thank-you" class="slide level2">
<h2>Thank you!</h2>
<div class="footer footer-default">

</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="presentation_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
    <script src="presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
    <script src="presentation_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
    <script src="presentation_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
    <script src="presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
    <script src="presentation_files/libs/revealjs/plugin/multiplex/socket.io.js"></script>
    <script src="presentation_files/libs/revealjs/plugin/multiplex/multiplex.js"></script>
    <script src="presentation_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  
  <script src="presentation_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="presentation_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="presentation_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="presentation_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 0.3,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'multiplex': {"secret":"16544188428532219174","id":"bdd8542e738e1024","url":"https://reveal-multiplex.glitch.me/"},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        function fireSlideChanged(previousSlide, currentSlide) {

          // dispatch for htmlwidgets
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for reveal
        if (window.Reveal) {
          window.Reveal.addEventListener("slidechanged", function(event) {
            fireSlideChanged(event.previousSlide, event.currentSlide);
          });
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        setTimeout(function() {
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          let href = ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const cites = ref.parentNode.getAttribute('data-cites').split(' ');
        tippyHover(ref, function() {
          var popup = window.document.createElement('div');
          cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    });
    </script>
    

</body></html>